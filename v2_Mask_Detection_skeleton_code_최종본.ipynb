{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVuAtM1g9b9W"
      },
      "source": [
        "\n",
        "# **0. Mask augmentation**\n",
        "\n",
        "*   mount google drive\n",
        "*   MaskTheFace github (https://github.com/aqeelanwar/MaskTheFace.git)\n",
        "*   include mask augmented face image samples in report\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW3NGlMU9UKG",
        "outputId": "40580daf-52c1-4966-f7e6-58faacf41daf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# google drive mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drive mount 후 colab의 현재 경로 /content\n",
        "# ! git clone https://github.com/aqeelanwar/MaskTheFace.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CelSWL20zIa9"
      },
      "outputs": [],
      "source": [
        "# WANDB 설치\n",
        "! pip install wandb\n",
        "\n",
        "# MaskTheFace Package 설치\n",
        "# ! pip install dlib\n",
        "# ! pip install face-recognition\n",
        "# ! pip install face-recognition-models\n",
        "# ! pip install dotmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # import packages\n",
        "# from pathlib import Path\n",
        "# import os, shutil\n",
        "\n",
        "# # DATA_URL = \"mask_data\" # 구글 드라이브에서의 폴더 이름\n",
        "# ZIP_FILENAME = \"data_original.zip\" # 폴더 안에 있는 data.zip 원본 파일 이름\n",
        "# UNZIP_DESTINATION = \"dataset\" # Colab VM에 저장될 폴더 이름\n",
        "# NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "# DRIVE_URL = Path('/content/drive/MyDrive')\n",
        "# ORIGINAL_DATA_URL = DRIVE_URL / ZIP_FILENAME\n",
        "\n",
        "# BASE_URL = Path(\"/content\")\n",
        "\n",
        "# if not (BASE_URL / UNZIP_DESTINATION).exists():\n",
        "#   (BASE_URL / UNZIP_DESTINATION).mkdir()\n",
        "\n",
        "# if not (BASE_URL / ZIP_FILENAME).exists() and ORIGINAL_DATA_URL.exists():\n",
        "#   shutil.copy2(ORIGINAL_DATA_URL, BASE_URL)\n",
        "\n",
        "# if (BASE_URL / ZIP_FILENAME).exists():\n",
        "#   os.system(f\"unzip {ZIP_FILENAME} -d {UNZIP_DESTINATION}\")\n",
        "#   (BASE_URL / ZIP_FILENAME).unlink()\n",
        "\n",
        "# BASE_URL = BASE_URL / UNZIP_DESTINATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cd 'MaskTheFace'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! python mask_the_face.py --path \"/content/dataset/train/not_wearing_mask\" --mask_type \"random\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! python mask_the_face.py --path \"/content/dataset/val/not_wearing_mask\" --mask_type \"random\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# directories = {\n",
        "#     \"train\": {\n",
        "#         \"masked\": \"/content/dataset/train/not_wearing_mask_masked\",\n",
        "#         \"wearing\": \"/content/dataset/train/wearing_mask\"\n",
        "#     },\n",
        "#     \"val\": {\n",
        "#         \"masked\": \"/content/dataset/val/not_wearing_mask_masked\",\n",
        "#         \"wearing\": \"/content/dataset/val/wearing_mask\"\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# for key, paths in directories.items():\n",
        "#     SOURCE = paths[\"masked\"]\n",
        "#     DESTINATION = paths[\"wearing\"]\n",
        "\n",
        "#     if os.path.exists(SOURCE):\n",
        "#         shutil.rmtree(DESTINATION)\n",
        "\n",
        "#     os.rename(SOURCE, DESTINATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpHtE4Lu96GR"
      },
      "source": [
        "# **1. Prepare Data for Training**\n",
        "\n",
        "*   data_loader using *torchvision.datasets.ImageFolder* for Custom dataset\n",
        "*   **image augmentation** in *transforms*\n",
        "*   include augmented face image samples in report\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHKJq22C9_Pk"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "# Pathlib : 파일 경로를 다루는데 사용하는 라이브러리\n",
        "# shutil : 파일을 복사하거나 이동하는데 사용하는 라이브러리\n",
        "# torch : 파이토치 라이브러리\n",
        "# torchvision : 파이토치에서 제공하는 이미지 관련 라이브러리\n",
        "# tqdm : 진행상황을 보여주는 라이브러리\n",
        "# sklearn : 사이킷런 \n",
        "\n",
        "from pathlib import Path\n",
        "import os, shutil\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "\n",
        "# DATASET 들 경로 설정 및 압축 해제\n",
        "\n",
        "DATA_URL = \"mask_data\" # 구글 드라이브에서의 폴더 이름\n",
        "ZIP_FILENAME = \"data.zip\" # 폴더 안에 있는 zip 파일 이름\n",
        "UNZIP_DESTINATION = \"dataset\" # Colab VM에 저장될 폴더 이름\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "DRIVE_URL = Path('/content/drive/MyDrive') / DATA_URL\n",
        "ORIGINAL_DATA_URL = DRIVE_URL / ZIP_FILENAME\n",
        "\n",
        "BASE_URL = Path(\"/content\")\n",
        "\n",
        "if not (BASE_URL / UNZIP_DESTINATION).exists():\n",
        "  (BASE_URL / UNZIP_DESTINATION).mkdir()\n",
        "\n",
        "if not (BASE_URL / ZIP_FILENAME).exists() and ORIGINAL_DATA_URL.exists():\n",
        "  shutil.copy2(ORIGINAL_DATA_URL, BASE_URL)\n",
        "\n",
        "if (BASE_URL / ZIP_FILENAME).exists():\n",
        "  os.system(f\"unzip {ZIP_FILENAME} -d {UNZIP_DESTINATION}\")\n",
        "  (BASE_URL / ZIP_FILENAME).unlink()\n",
        "\n",
        "drive.flush_and_unmount()\n",
        "\n",
        "BASE_URL = BASE_URL / UNZIP_DESTINATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGgwKyEY9_SP"
      },
      "outputs": [],
      "source": [
        "# transforms for image augmentation\n",
        "train_transform = transforms.v2.Compose([\n",
        "    v2.CenterCrop(112),\n",
        "    v2.RandomHorizontalFlip(),\n",
        "    v2.RandomVerticalFlip(),\n",
        "    v2.RandomRotation(degrees=(0, 180)),\n",
        "    # v2.ColorJitter(),\n",
        "    # v2.Grayscale(num_output_channels=3),\n",
        "    # v2.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.)),\n",
        "    v2.RandomPerspective(distortion_scale=0.6, p=1.0),\n",
        "    # scale=True의 의미: https://pytorch.org/vision/main/transforms.html#dtype-and-expected-value-range\n",
        "    # v2를 사용한 이유: https://pytorch.org/vision/main/transforms.html#v1-or-v2-which-one-should-i-use\n",
        "    # v2를 사용하면 ToTensor 대신 ToImage와 ToDtype의 조합 사용해야함.\n",
        "    v2.ToImage(), v2.ToDtype(torch.float32, scale=True), # scale=True -> [0, 1] 사이로 값 조정\n",
        "])\n",
        "\n",
        "val_transform = transforms.v2.Compose([\n",
        "    v2.CenterCrop(112),\n",
        "    v2.RandomHorizontalFlip(),\n",
        "    v2.RandomVerticalFlip(),\n",
        "    v2.RandomRotation(degrees=(0, 180)),\n",
        "    # v2.ColorJitter(),\n",
        "    # v2.Grayscale(num_output_channels=3),\n",
        "    # v2.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.)),\n",
        "    v2.RandomPerspective(distortion_scale=0.6, p=1.0),\n",
        "    v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n",
        "])\n",
        "\n",
        "test_transform = transforms.v2.Compose([\n",
        "    v2.Resize(128),\n",
        "    v2.CenterCrop(112),\n",
        "    v2.RandomHorizontalFlip(),\n",
        "    v2.RandomVerticalFlip(),\n",
        "    v2.RandomRotation(degrees=(0, 180)),\n",
        "    # v2.ColorJitter(),\n",
        "    # v2.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.)),\n",
        "    v2.RandomPerspective(distortion_scale=0.6, p=1.0),\n",
        "    v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiQqnEio9_U1"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "train_path = BASE_URL / 'train'\n",
        "val_path = BASE_URL / 'val'\n",
        "test_path = BASE_URL / 'test'\n",
        "\n",
        "# write ImageFolder code below\n",
        "train_dataset = ImageFolder(train_path, transform=train_transform)\n",
        "val_dataset = ImageFolder(val_path, transform=val_transform)\n",
        "test_dataset = ImageFolder(test_path, transform=test_transform)\n",
        "\n",
        "# check the label\n",
        "# train_data.class_to_idx -> {'not_wearing_mask': 0, 'wearing_mask': 1}\n",
        "# val_data.class_to_idx -> {'not_wearing_mask': 0, 'wearing_mask': 1}\n",
        "print(f\"{train_path} : {train_dataset.class_to_idx}\")\n",
        "print(f\"{val_path}: {val_dataset.class_to_idx}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def imshow(img, ax=None, title=None, fontsize=8):\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    img = img / 2 + 0.5  # unnormalize if normalization was applied\n",
        "    npimg = img.numpy()\n",
        "    ax.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    if title is not None:\n",
        "        ax.set_title(title, fontsize=fontsize)\n",
        "    ax.axis('off')\n",
        "\n",
        "# 이미지와 라벨 출력\n",
        "def show_images(dataset, predicted_labels=None, num_images_per_class=4):\n",
        "    class_indices = {class_name: [] for class_name in dataset.class_to_idx.keys()}\n",
        "\n",
        "    for idx, (img, label) in enumerate(dataset):\n",
        "        class_name = list(dataset.class_to_idx.keys())[label]\n",
        "        if len(class_indices[class_name]) < num_images_per_class:\n",
        "            class_indices[class_name].append(idx)\n",
        "        if all(len(indices) == num_images_per_class for indices in class_indices.values()):\n",
        "            break\n",
        "\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(6, 3))\n",
        "    axes = axes.flatten()\n",
        "    for i, class_name in enumerate(class_indices.keys()):\n",
        "        for j, idx in enumerate(class_indices[class_name]):\n",
        "            img, label = dataset[idx]\n",
        "            title = f\"{class_name} : {label}\"\n",
        "\n",
        "            if predicted_labels is not None:\n",
        "              predicted_label = predicted_labels[idx]\n",
        "              title += f\"\\nprediction : {predicted_label}\"\n",
        "            ax = axes[i * num_images_per_class + j]\n",
        "            imshow(img, ax=ax, title=title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 이미지 출력\n",
        "show_images(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3_M5PPK-rpS"
      },
      "source": [
        "# **2. Prepare Model**\n",
        "\n",
        "*   Pytorch ResNet - *ref*. https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n",
        "*   use **ResNet50** from torchvision.model_zoo\n",
        "*   explore more models in https://pytorch.org/vision/stable/models.html\n",
        "*   **change the dimension of the classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnwmCewA9_XZ",
        "outputId": "3c93bf31-6b38-4858-9a1d-b5c221ed9a31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# assign device cpu or gpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWtoPoMw9_aB"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights # for pretrained\n",
        "\n",
        "def initialize_weights(m):\n",
        "  if isinstance(m, nn.Conv2d):\n",
        "    nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
        "  elif isinstance(m, nn.BatchNorm2d):\n",
        "    nn.init.constant_(m.weight.data, 1)\n",
        "    nn.init.constant_(m.bias.data, 0)\n",
        "  elif isinstance(m, nn.Linear):\n",
        "    nn.init.kaiming_uniform_(m.weight.data)\n",
        "    nn.init.constant_(m.bias.data, 0)\n",
        "  elif isinstance(m, nn.Sequential):\n",
        "    for sm in list(m):\n",
        "      initialize_weights(sm)\n",
        "  elif isinstance(m, models.resnet.Bottleneck):\n",
        "    for sm in m.children():\n",
        "      initialize_weights(sm)\n",
        "\n",
        "def init_resnet50(init_weights = False):\n",
        "  model = models.resnet50(weights = ResNet50_Weights.IMAGENET1K_V2) #pretrain model = ResNet50_Weights.IMAGENET1K_V2\n",
        "  model.fc = nn.Linear(model.fc.in_features, 1) # change the # of classes\n",
        "\n",
        "  if init_weights:\n",
        "    model.apply(initialize_weights)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX0zF19P9_cd"
      },
      "outputs": [],
      "source": [
        "# load model and change the # of classes\n",
        "model = init_resnet50()\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVE3zX9_-_01"
      },
      "source": [
        "# **3. Training**\n",
        "\n",
        "\n",
        "*   write **training code** including belows:\n",
        "   - hyper parameters such as batch size, learning rate, epoch\n",
        "   - criterion(loss function such as BCELoss), optimizer(eg. Adam, SGD, etc.)  and scheduler\n",
        "   - save model weight\n",
        "\n",
        "*   **print training/validation loss and accuracy** per epoch or iteration\n",
        "*   inlcude visualizer, **tensorboard**, to show training/validation accuracy and loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GSGazJ_9_hX",
        "outputId": "90ed8eeb-b709-4457-996e-a344dce6b8e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "import datetime\n",
        "import os\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "WANDB_KEY = \"8aa54cefa31fe5992ce2d2969f979237aae9f81a\"\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "wandb.login(key=WANDB_KEY)\n",
        "\n",
        "def init_wandb(project_name = \"ICPBL_REPORT\", name=\"test\", config = None):\n",
        "  if config is None:\n",
        "    config = {\n",
        "      \"architecture\": \"ResNet50\",\n",
        "      \"learning_rate\": 0.01,\n",
        "      \"batch_size\": 64,\n",
        "      \"epochs\": 50,\n",
        "      \"weight_decay\": 0,\n",
        "      \"momentum\": 0.93,\n",
        "      \"init_weights\": False,\n",
        "      \"optimizer\": 'SGD',\n",
        "      \"lr_scheduler\": \"CosineAnnealing\",\n",
        "    }\n",
        "\n",
        "  logger = wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=project_name,\n",
        "    name = name,\n",
        "    # track hyperparameters and run metadata\n",
        "    config=config\n",
        "  )\n",
        "\n",
        "  return logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQJGdT_weclU"
      },
      "outputs": [],
      "source": [
        "# utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_labels_from_probs(y_hat):\n",
        "  return torch.where(F.sigmoid(y_hat.reshape(-1)) > 0.5, 1, 0)\n",
        "\n",
        "def get_acc(y_hat, labels):\n",
        "  if torch.is_floating_point(y_hat):\n",
        "    y_hat = get_labels_from_probs(y_hat)\n",
        "\n",
        "  return y_hat.eq(labels).sum().item()\n",
        "\n",
        "def get_roc_curve_data(labels, y_hat):\n",
        "  y_hat = y_hat.reshape(-1)\n",
        "  fprs, tprs, _ = metrics.roc_curve(labels, y_hat)\n",
        "  roc_auc = metrics.roc_auc_score(labels, y_hat)\n",
        "\n",
        "  return fprs, tprs, roc_auc\n",
        "\n",
        "def plot_roc_curve(fprs, tprs, roc_auc):\n",
        "  # sklearn 으로 ROC Curve 그리기\n",
        "  # ROC Curve를 plot 곡선으로 그림.\n",
        "  plt.plot(fprs , tprs, label=f'ROC (AUC: {roc_auc:.4f})')\n",
        "  # 가운데 대각선 직선을 그림.\n",
        "  plt.plot([0, 1], [0, 1], 'k:', label='Random')\n",
        "  plt.title('ROC curve')\n",
        "  plt.xlabel('FPR')\n",
        "  plt.ylabel('TPR')\n",
        "  plt.legend(loc='best')\n",
        "  plt.savefig('ROC.png',dpi=300);\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntTKQS0m9_ox"
      },
      "outputs": [],
      "source": [
        "def val_model(model, criterion, dataloader, use_sklearn = False, plot_mask=False):\n",
        "  # write validation code\n",
        "  if model.training:\n",
        "    model.eval()\n",
        "\n",
        "  accuracy = 0.0\n",
        "  loss = 0.0\n",
        "\n",
        "  global_y_hat = torch.Tensor()\n",
        "  global_y_hat.requires_grad = False\n",
        "  global_labels = torch.Tensor()\n",
        "  global_labels.requires_grad = False\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images, labels in tqdm(dataloader, total=len(dataloader)):\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      y_hat = model(images)\n",
        "      y_hat = y_hat.reshape(-1)\n",
        "\n",
        "      _l = criterion(y_hat, labels.type(dtype=torch.float32))\n",
        "      loss += _l.item()\n",
        "\n",
        "      accuracy += get_acc(y_hat, labels)\n",
        "\n",
        "      global_y_hat = torch.cat([global_y_hat, y_hat.cpu()], dim=-1)\n",
        "      global_labels = torch.cat([global_labels, labels.cpu()], dim=-1)\n",
        "\n",
        "  loss = float(\"{:.4f}\".format(loss / len(dataloader)))\n",
        "  accuracy = float(\"{:.4f}\".format(accuracy / len(dataloader.dataset)))\n",
        "\n",
        "  if plot_mask is True:\n",
        "    predicted_labels = get_labels_from_probs(global_y_hat)\n",
        "    show_images(dataloader.dataset, predicted_labels)\n",
        "\n",
        "  fprs, tprs, auc = get_roc_curve_data(global_labels, global_y_hat)\n",
        "\n",
        "  if use_sklearn:\n",
        "    plot_roc_curve(fprs, tprs, auc)\n",
        "\n",
        "  global_y_hat = torch.stack([torch.zeros_like(global_y_hat), global_y_hat], dim=1)\n",
        "  roc = wandb.plot.roc_curve(global_labels, global_y_hat, labels=[\"not_wearing_mask\", \"wearing_mask\"])\n",
        "\n",
        "  return accuracy, loss, auc, roc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDqbC5k09_mS"
      },
      "outputs": [],
      "source": [
        "def train_model(model, optim, criterion, dataloader, save_model=True):\n",
        "  if not model.training:\n",
        "      model.train()\n",
        "\n",
        "  accuracy = 0.0\n",
        "  loss = 0.0\n",
        "\n",
        "  global_y_hat = torch.Tensor()\n",
        "  global_labels = torch.Tensor()\n",
        "\n",
        "  for images, labels in tqdm(dataloader, total=len(dataloader)):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    y_hat = model(images)\n",
        "    y_hat = y_hat.reshape(-1)\n",
        "\n",
        "    _l = criterion(y_hat, labels.type(dtype=torch.float32))\n",
        "\n",
        "    optim.zero_grad()\n",
        "    _l.backward()\n",
        "    optim.step()\n",
        "\n",
        "    loss += _l\n",
        "    accuracy += get_acc(y_hat, labels)\n",
        "\n",
        "    global_y_hat = torch.cat([global_y_hat, y_hat.cpu()], dim=-1)\n",
        "    global_labels = torch.cat([global_labels, labels.cpu()], dim=-1)\n",
        "\n",
        "  loss = float(\"{:.4f}\".format(loss / len(dataloader)))\n",
        "  accuracy = float(\"{:.4f}\".format(accuracy / len(dataloader.dataset)))\n",
        "\n",
        "  global_y_hat = get_labels_from_probs(global_y_hat)\n",
        "  f1_score = metrics.f1_score(global_y_hat, global_labels)\n",
        "  f1_score = float(\"{:.4f}\".format(f1_score))\n",
        "\n",
        "  return accuracy, loss, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PteSbpql9_ts"
      },
      "outputs": [],
      "source": [
        "# hyper parameters\n",
        "config = {\n",
        "    \"architecture\": \"ResNet50\",\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 50,\n",
        "    \"weight_decay\": 0,\n",
        "    \"momentum\": 0.93,\n",
        "    \"init_weights\": False,\n",
        "    \"optimizer\": 'SGD',\n",
        "    \"scheduler_type\": '',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tX_gJkuhCKov"
      },
      "outputs": [],
      "source": [
        "# optimizer, loss, scheduler\n",
        "model = init_resnet50(init_weights=config[\"init_weights\"])\n",
        "model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "if config[\"optimizer\"] == 'SGD':\n",
        "  optimizer = optim.SGD(model.parameters(), lr=config[\"learning_rate\"], momentum=config[\"momentum\"])\n",
        "elif config[\"optimizer\"] == 'Adam':\n",
        "  optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "else:\n",
        "  pass\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "criterion.to(device)\n",
        "\n",
        "# lr_scheduler\n",
        "lr_scheduler = None\n",
        "if config[\"scheduler_type\"] == 'StepLR':\n",
        "  lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma=0.8)\n",
        "elif config[\"scheduler_type\"] == 'CyclicLR':\n",
        "  lr_scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.005, max_lr=0.015, step_size_up=5, step_size_down=5, mode='triangular')\n",
        "elif config[\"scheduler_type\"] == 'CosineAnnealingLR':\n",
        "  lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min = 0.001)\n",
        "else:\n",
        "  pass\n",
        "\n",
        "print(lr_scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xNOJab5CKrE"
      },
      "outputs": [],
      "source": [
        "# data_loader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=NUM_WORKERS, pin_memory=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=NUM_WORKERS, pin_memory=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8gaKCGC9_rW"
      },
      "outputs": [],
      "source": [
        "# training and validation code can be writed in one function. It's your taste!\n",
        "from copy import deepcopy\n",
        "import json\n",
        "\n",
        "logger = init_wandb(project_name=\"ResNet50\", config=config)\n",
        "\n",
        "best_f1_score = 0.0\n",
        "best_f1_model = None\n",
        "best_model_metadata = None\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "  train_acc, train_loss, f1_score = train_model(model, optim=optimizer, criterion=criterion, dataloader=train_dataloader)\n",
        "  val_acc, val_loss, auc, roc = val_model(model, criterion=criterion, dataloader=val_dataloader)\n",
        "\n",
        "  logger.log({\"train_acc\": train_acc}, step=epoch)\n",
        "  logger.log({\"train_loss\": train_loss}, step=epoch)\n",
        "  logger.log({\"val_acc\": val_acc}, step=epoch)\n",
        "  logger.log({\"val_loss\": val_loss}, step=epoch)\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    logger.log({\"roc\": roc, \"auc\": auc})\n",
        "\n",
        "  if f1_score > best_f1_score:\n",
        "    best_f1_score = f1_score\n",
        "    best_f1_model = deepcopy(model.state_dict())\n",
        "    best_model_metadata = {\n",
        "        \"epoch\": epoch\n",
        "    }\n",
        "\n",
        "  print(\"\")\n",
        "  print(f\"{'Epoch':<20}:  {epoch+1}\")\n",
        "  print(f\"{'Train Accuracy':<20}: {train_acc}\")\n",
        "  print(f\"{'Train Loss':<20}: {train_loss}\")\n",
        "  print(f\"{'Val Accuracy':<20}: {val_acc}\")\n",
        "  print(f\"{'Val Loss':<20}: {val_loss}\")\n",
        "  print(\"\")\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "save_model_path = Path(\"./best_model.pt\")\n",
        "\n",
        "torch.save(best_f1_model, save_model_path)\n",
        "with open(Path(\"./best_model_metadata.json\"), 'w', encoding=\"utf-8\") as f:\n",
        "  json.dump(best_model_metadata, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_acc, val_loss, auc, roc = val_model(model, criterion=criterion, dataloader=val_dataloader, plot_mask=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V62h0NkvfDtE"
      },
      "outputs": [],
      "source": [
        "# Evaluation code with our trained model\n",
        "# ROC curve, AUC (Hint: use sklearn or wandb function, using sklearn to extract fpr, tpr will be bonus score)\n",
        "model = init_resnet50()\n",
        "model.load_state_dict(torch.load(save_model_path))\n",
        "model.to(device)\n",
        "test_acc, test_loss, roc_auc, roc = val_model(model, criterion=criterion, dataloader=test_dataloader, use_sklearn=True, plot_mask=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(test_acc)\n",
        "\n",
        "# 22 -> init_weight+Cos+0.93(22) 0.79\n",
        "# 23 -> weight_decay = 0.0001 , momentum = 0.93 -> 0.7949\n",
        "# 25 -> weight_decay = 0.0001 , momentum = 0.93, dropout = 0.5 -> 80.94\n",
        "# drop_test (cosineannealingLR): 78.9퍼\n",
        "# drop_test (stepLR) : 75퍼\n",
        "# default_Step : 76퍼"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
